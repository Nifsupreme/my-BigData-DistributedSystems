{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark assignment 2: Collocations\n",
    "As for the second part of the assignment, your task is to extract collocations: that is word combinations that occur together. For example, “high school” or “roman empire”.\n",
    "\n",
    "To find collocations, you will use NPMI (normalized pointwise mutual information) metric.\n",
    "\n",
    "PMI of two words, a & b, is defined as\n",
    "\n",
    "    “PMI(a, b) = ln (P(ab) / (P(a) * P(b))”\n",
    "   \n",
    "where:\n",
    "    - P(ab) is the probability of two words coming one after the other\n",
    "    - P(a) and P(b) are probabilities of words a & b respectively.\n",
    "\n",
    "You will estimate probabilities with occurrence counts, that is\n",
    "    - “P(a) = # of occurrences of word a / total number of words”\n",
    "    - “P(ab) = # of occurrences of words ‘a b’ / total number of word pairs”.\n",
    "\n",
    "To build an intuition behind the definition, see Reading material.\n",
    "\n",
    "Therefore, rare combinations of coupled words have large PMI.\n",
    "\n",
    "NPMI is computed as \n",
    "    \n",
    "    “NPMI(a, b) = PMI(a, b) / -ln P(ab)”. This normalizes the quantity to be within the range [-1; 1].\n",
    "\n",
    "You task is a bit more complicated now:\n",
    "\n",
    "- Extract all the words, as in the previous task.\n",
    "- Filter out stopwords using the dictionary (/datasets/stop_words_en.txt ) (do not forget to convert words to the lowercase!)\n",
    "- Compute all bigrams (that is, pairs of consequent words)\n",
    "- Leave only bigrams with at least 500 occurrences\n",
    "- Compute NPMI for every bigram (note: when computing probabilities, you need unpruned counts!)\n",
    "- Sort word pairs by NPMI in the descending order\n",
    "- Print top 39 word pairs, with words delimited by the underscore “_”\n",
    "\n",
    "For example,\n",
    "\n",
    "    roman_empire\n",
    "\n",
    "    south_africa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"MyApp\").setMaster(\"local\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = unicode(line.rstrip()).split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text, flags=re.UNICODE)\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "wiki = sc.textFile(\"/data/wiki/en_articles_part/articles-part\", 16).map(parse_article).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = wiki.take(1)[0]\n",
    "# for word in result[:50]:\n",
    "#     print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = '/datasets/stop_words_en.txt'\n",
    "def read_stop_words(file_path):\n",
    "    return set(word.strip().lower() for word in open(file_path))\n",
    "stop_words = read_stop_words(stop_words_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = wiki.flatMap(lambda x: [(x[i].lower(), 1) for i in range(len(x)-1)])\n",
    "total_words = float(words.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words \\\n",
    "    .filter(lambda (x, y): x not in stop_words) \\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_words = words.map(lambda (x, y): (x, float(y)/ total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = wiki.flatMap(lambda x: [((x[i].lower(), x[i+1].lower()), 1) for i in range(len(x)-1)])\n",
    "total_pairs = float(pairs.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs \\\n",
    "    .filter(lambda (x,y): (x[0] not in stop_words) and (x[1] not in stop_words)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .filter(lambda (x,y): y >= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pairs = pairs.map(lambda (x, y): (x, float(y)/ total_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = P_pairs.map(lambda ((a,b), P_ab): (a, (b, P_ab)))\n",
    "pairs = pairs \\\n",
    "    .leftOuterJoin(P_words) \\\n",
    "    .map(lambda (a, ((b, P_ab), P_a)): (b, (a, P_a, P_ab))) \\\n",
    "    .leftOuterJoin(P_words) \\\n",
    "    .map(lambda (b, ((a, P_a, P_ab), P_b)): ((a, b), (P_a, P_b, P_ab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "PMI = pairs.map(lambda ((a, b), (P_a, P_b, P_ab)): ((a, b), (np.log(P_ab/(P_a*P_b)), P_ab)))\n",
    "NPMI = PMI.map(lambda ((a, b), (PMI_ab, P_ab)): ((a, b), PMI_ab/np.log(P_ab)))\n",
    "NPMI = NPMI.sortBy(lambda (x,y): y, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north_american\n",
      "years_later\n",
      "american_football\n",
      "york_city\n",
      "american_baseball\n",
      "american_actress\n",
      "american_actor\n",
      "high_school\n",
      "american_singer-songwriter\n",
      "took_place\n",
      "united_nations\n",
      "roman_empire\n",
      "south_africa\n",
      "war_ii\n",
      "catholic_church\n",
      "world_war\n",
      "civil_war\n",
      "new_zealand\n",
      "north_america\n",
      "notes_references\n",
      "roman_catholic\n",
      "united_kingdom\n",
      "university_press\n",
      "baseball_player\n",
      "air_force\n",
      "soviet_union\n",
      "references_external\n",
      "20th_century\n",
      "19th_century\n",
      "supreme_court\n",
      "new_york\n",
      "et_al\n",
      "san_francisco\n",
      "prime_minister\n",
      "united_states\n",
      "external_links\n",
      "los_angeles\n"
     ]
    }
   ],
   "source": [
    "results = NPMI.take(39)\n",
    "for result in results:\n",
    "    print '%s_%s' % (result[0][0], result[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "minh tran phuong truong minh xuan thao phuong tran phuong cham phuong tran phuong xuan thao minh tam cham cham vy vy minh tran phuong cham cham cham cham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stop_words.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stop_words.txt\n",
    "minh\n",
    "tam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = 'stop_words.txt'\n",
    "def read_stop_words(file_path):\n",
    "    return set(word.strip().lower() for word in open(file_path))\n",
    "stop_words = read_stop_words(stop_words_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'minh',\n",
       "  u'tran',\n",
       "  u'phuong',\n",
       "  u'truong',\n",
       "  u'minh',\n",
       "  u'xuan',\n",
       "  u'thao',\n",
       "  u'phuong',\n",
       "  u'tran',\n",
       "  u'phuong',\n",
       "  u'cham',\n",
       "  u'phuong',\n",
       "  u'tran',\n",
       "  u'phuong',\n",
       "  u'xuan',\n",
       "  u'thao',\n",
       "  u'minh',\n",
       "  u'tam',\n",
       "  u'cham',\n",
       "  u'cham',\n",
       "  u'vy',\n",
       "  u'vy',\n",
       "  u'minh',\n",
       "  u'tran',\n",
       "  u'phuong',\n",
       "  u'cham',\n",
       "  u'cham',\n",
       "  u'cham',\n",
       "  u'cham']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalizeWords(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "test = sc.textFile(\"test.txt\", 16).map(normalizeWords).cache()\n",
    "test.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'minh', 1),\n",
       " (u'tran', 1),\n",
       " (u'phuong', 1),\n",
       " (u'truong', 1),\n",
       " (u'minh', 1),\n",
       " (u'xuan', 1),\n",
       " (u'thao', 1),\n",
       " (u'phuong', 1),\n",
       " (u'tran', 1),\n",
       " (u'phuong', 1),\n",
       " (u'cham', 1),\n",
       " (u'phuong', 1),\n",
       " (u'tran', 1),\n",
       " (u'phuong', 1),\n",
       " (u'xuan', 1),\n",
       " (u'thao', 1),\n",
       " (u'minh', 1),\n",
       " (u'tam', 1),\n",
       " (u'cham', 1),\n",
       " (u'cham', 1),\n",
       " (u'vy', 1),\n",
       " (u'vy', 1),\n",
       " (u'minh', 1),\n",
       " (u'tran', 1),\n",
       " (u'phuong', 1),\n",
       " (u'cham', 1),\n",
       " (u'cham', 1),\n",
       " (u'cham', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = test.flatMap(lambda x: [(x[i].lower(), 1) for i in range(len(x)-1)]) \n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = float(words.count())\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'truong', 1),\n",
       " (u'thao', 2),\n",
       " (u'cham', 6),\n",
       " (u'xuan', 2),\n",
       " (u'tran', 4),\n",
       " (u'vy', 2),\n",
       " (u'phuong', 6)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = words \\\n",
    "    .filter(lambda (x, y): x not in stop_words) \\\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'truong', 0.03571428571428571),\n",
       " (u'thao', 0.07142857142857142),\n",
       " (u'cham', 0.21428571428571427),\n",
       " (u'xuan', 0.07142857142857142),\n",
       " (u'tran', 0.14285714285714285),\n",
       " (u'vy', 0.07142857142857142),\n",
       " (u'phuong', 0.21428571428571427)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_words = words.map(lambda (x, y): (x, float(y)/ total_words))\n",
    "P_words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'minh', u'tran'), 1),\n",
       " ((u'tran', u'phuong'), 1),\n",
       " ((u'phuong', u'truong'), 1),\n",
       " ((u'truong', u'minh'), 1),\n",
       " ((u'minh', u'xuan'), 1),\n",
       " ((u'xuan', u'thao'), 1),\n",
       " ((u'thao', u'phuong'), 1),\n",
       " ((u'phuong', u'tran'), 1),\n",
       " ((u'tran', u'phuong'), 1),\n",
       " ((u'phuong', u'cham'), 1),\n",
       " ((u'cham', u'phuong'), 1),\n",
       " ((u'phuong', u'tran'), 1),\n",
       " ((u'tran', u'phuong'), 1),\n",
       " ((u'phuong', u'xuan'), 1),\n",
       " ((u'xuan', u'thao'), 1),\n",
       " ((u'thao', u'minh'), 1),\n",
       " ((u'minh', u'tam'), 1),\n",
       " ((u'tam', u'cham'), 1),\n",
       " ((u'cham', u'cham'), 1),\n",
       " ((u'cham', u'vy'), 1),\n",
       " ((u'vy', u'vy'), 1),\n",
       " ((u'vy', u'minh'), 1),\n",
       " ((u'minh', u'tran'), 1),\n",
       " ((u'tran', u'phuong'), 1),\n",
       " ((u'phuong', u'cham'), 1),\n",
       " ((u'cham', u'cham'), 1),\n",
       " ((u'cham', u'cham'), 1),\n",
       " ((u'cham', u'cham'), 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = test.flatMap(lambda x: [((x[i].lower(), x[i+1].lower()), 1) for i in range(len(x)-1)])\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_pairs = float(pairs.count())\n",
    "total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'phuong', u'tran'), 2),\n",
       " ((u'tran', u'phuong'), 4),\n",
       " ((u'cham', u'cham'), 4),\n",
       " ((u'phuong', u'cham'), 2),\n",
       " ((u'xuan', u'thao'), 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = pairs \\\n",
    "    .filter(lambda (x,y): (x[0] not in stop_words) and (x[1] not in stop_words)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .filter(lambda (x,y): y >= 2)\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'phuong', u'tran'), 0.07142857142857142),\n",
       " ((u'tran', u'phuong'), 0.14285714285714285),\n",
       " ((u'cham', u'cham'), 0.14285714285714285),\n",
       " ((u'phuong', u'cham'), 0.07142857142857142),\n",
       " ((u'xuan', u'thao'), 0.07142857142857142)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_pairs = pairs.map(lambda (x, y): (x, float(y)/ total_pairs))\n",
    "P_pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'phuong', (u'tran', 0.07142857142857142)),\n",
       " (u'tran', (u'phuong', 0.14285714285714285)),\n",
       " (u'cham', (u'cham', 0.14285714285714285)),\n",
       " (u'phuong', (u'cham', 0.07142857142857142)),\n",
       " (u'xuan', (u'thao', 0.07142857142857142))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = P_pairs.map(lambda ((a,b), P_ab): (a, (b, P_ab)))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'tran', ((u'phuong', 0.14285714285714285), 0.14285714285714285)),\n",
       " (u'cham', ((u'cham', 0.14285714285714285), 0.21428571428571427)),\n",
       " (u'xuan', ((u'thao', 0.07142857142857142), 0.07142857142857142)),\n",
       " (u'phuong', ((u'tran', 0.07142857142857142), 0.21428571428571427)),\n",
       " (u'phuong', ((u'cham', 0.07142857142857142), 0.21428571428571427))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = pairs.leftOuterJoin(P_words)\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'phuong', (u'tran', 0.14285714285714285, 0.14285714285714285)),\n",
       " (u'cham', (u'cham', 0.21428571428571427, 0.14285714285714285)),\n",
       " (u'thao', (u'xuan', 0.07142857142857142, 0.07142857142857142)),\n",
       " (u'tran', (u'phuong', 0.21428571428571427, 0.07142857142857142)),\n",
       " (u'cham', (u'phuong', 0.21428571428571427, 0.07142857142857142))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = pairs.map(lambda (a, ((b, P_ab), P_a)): (b, (a, P_a, P_ab)))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'thao',\n",
       "  ((u'xuan', 0.07142857142857142, 0.07142857142857142), 0.07142857142857142)),\n",
       " (u'cham',\n",
       "  ((u'cham', 0.21428571428571427, 0.14285714285714285), 0.21428571428571427)),\n",
       " (u'cham',\n",
       "  ((u'phuong', 0.21428571428571427, 0.07142857142857142),\n",
       "   0.21428571428571427)),\n",
       " (u'tran',\n",
       "  ((u'phuong', 0.21428571428571427, 0.07142857142857142),\n",
       "   0.14285714285714285)),\n",
       " (u'phuong',\n",
       "  ((u'tran', 0.14285714285714285, 0.14285714285714285), 0.21428571428571427))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = pairs.leftOuterJoin(P_words)\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'xuan', u'thao'),\n",
       "  (0.07142857142857142, 0.07142857142857142, 0.07142857142857142)),\n",
       " ((u'cham', u'cham'),\n",
       "  (0.21428571428571427, 0.21428571428571427, 0.14285714285714285)),\n",
       " ((u'phuong', u'cham'),\n",
       "  (0.21428571428571427, 0.21428571428571427, 0.07142857142857142)),\n",
       " ((u'phuong', u'tran'),\n",
       "  (0.21428571428571427, 0.14285714285714285, 0.07142857142857142)),\n",
       " ((u'tran', u'phuong'),\n",
       "  (0.14285714285714285, 0.21428571428571427, 0.14285714285714285))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = pairs.map(lambda (b, ((a, P_a, P_ab), P_b)): ((a, b), (P_a, P_b, P_ab)))\n",
    "pairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'xuan', u'thao'), (2.6390573296152584, 0.07142857142857142)),\n",
       " ((u'cham', u'cham'), (1.1349799328389845, 0.14285714285714285)),\n",
       " ((u'phuong', u'cham'), (0.44183275227903923, 0.07142857142857142)),\n",
       " ((u'phuong', u'tran'), (0.84729786038720367, 0.07142857142857142)),\n",
       " ((u'tran', u'phuong'), (1.5404450409471491, 0.14285714285714285))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "PMI = pairs.map(lambda ((a, b), (P_a, P_b, P_ab)): ((a, b), (np.log(P_ab/(P_a*P_b)), P_ab)))\n",
    "PMI.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'xuan', u'thao'), -0.99999999999999978),\n",
       " ((u'cham', u'cham'), -0.58326430610888502),\n",
       " ((u'phuong', u'cham'), -0.16742067226840154),\n",
       " ((u'phuong', u'tran'), -0.32106080109700724),\n",
       " ((u'tran', u'phuong'), -0.79163215305444257)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPMI = PMI.map(lambda ((a, b), (PMI_ab, P_ab)): ((a, b), PMI_ab/np.log(P_ab)))\n",
    "NPMI.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'phuong', u'cham'), -0.16742067226840154),\n",
       " ((u'phuong', u'tran'), -0.32106080109700724),\n",
       " ((u'cham', u'cham'), -0.58326430610888502),\n",
       " ((u'tran', u'phuong'), -0.79163215305444257),\n",
       " ((u'xuan', u'thao'), -0.99999999999999978)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPMI = NPMI.sortBy(lambda (x,y): y, ascending=False)\n",
    "NPMI.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
