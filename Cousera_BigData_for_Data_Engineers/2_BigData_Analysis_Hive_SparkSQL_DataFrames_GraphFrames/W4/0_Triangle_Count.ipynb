{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Count\n",
    "![](./img/triangleCount.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeList = [(1,2), (1,3), (1,4), (2,3), (2,5), (3,4), (3,5), (3,6), (3,7)]\n",
    "graphData = sparkSession.sparkContext \\\n",
    "    .parallelize(edgeList) \\\n",
    "    .map(lambda(src, dst): Row(src, dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  1|  3|\n",
      "|  1|  4|\n",
      "|  2|  3|\n",
      "|  2|  5|\n",
      "|  3|  4|\n",
      "|  3|  5|\n",
      "|  3|  6|\n",
      "|  3|  7|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graphSchemaAB = StructType([\n",
    "    StructField('A', IntegerType(), nullable=False),\n",
    "    StructField('B', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "ab = sparkSession.createDataFrame(graphData, graphSchemaAB)\n",
    "ab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphSchemaBC1 = StructType([\n",
    "    StructField('B', IntegerType(), nullable=False),\n",
    "    StructField('C1', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "bc1 = sparkSession.createDataFrame(graphData, graphSchemaBC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphSchemaAC2 = StructType([\n",
    "    StructField('A', IntegerType(), nullable=False),\n",
    "    StructField('C2', StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "ac2 = sparkSession.createDataFrame(graphData, graphSchemaAC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  B|  A| C1|\n",
      "+---+---+---+\n",
      "|  3|  1|  4|\n",
      "|  3|  1|  5|\n",
      "|  3|  1|  6|\n",
      "|  3|  1|  7|\n",
      "|  3|  2|  4|\n",
      "|  3|  2|  5|\n",
      "|  3|  2|  6|\n",
      "|  3|  2|  7|\n",
      "|  2|  1|  3|\n",
      "|  2|  1|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abc1 = ab.join(bc1, 'B')\n",
    "abc1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B| C1| C2|\n",
      "+---+---+---+---+\n",
      "|  1|  3|  4|  2|\n",
      "|  1|  3|  4|  3|\n",
      "|  1|  3|  4|  4|\n",
      "|  1|  3|  5|  2|\n",
      "|  1|  3|  5|  3|\n",
      "|  1|  3|  5|  4|\n",
      "|  1|  3|  6|  2|\n",
      "|  1|  3|  6|  3|\n",
      "|  1|  3|  6|  4|\n",
      "|  1|  3|  7|  2|\n",
      "|  1|  3|  7|  3|\n",
      "|  1|  3|  7|  4|\n",
      "|  1|  2|  3|  2|\n",
      "|  1|  2|  3|  3|\n",
      "|  1|  2|  3|  4|\n",
      "|  1|  2|  5|  2|\n",
      "|  1|  2|  5|  3|\n",
      "|  1|  2|  5|  4|\n",
      "|  2|  3|  4|  3|\n",
      "|  2|  3|  4|  5|\n",
      "+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abc1c2 = abc1.join(ac2, 'A')\n",
    "abc1c2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|triangleVertex|count|\n",
      "+--------------+-----+\n",
      "|             1|    2|\n",
      "|             2|    2|\n",
      "|             3|    3|\n",
      "|             4|    1|\n",
      "|             5|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col, explode\n",
    "\n",
    "vertexTriangle = abc1c2 \\\n",
    "    .filter(\"C1 = C2\") \\\n",
    "    .select(\n",
    "        array(col('A'), col('B'), col('C1')).alias('triangleVertices')) \\\n",
    "    .select(\n",
    "        explode('triangleVertices').alias('triangleVertex')) \\\n",
    "    .groupBy('triangleVertex') \\\n",
    "    .count() \\\n",
    "    .orderBy('triangleVertex').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangle Count method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeListBC = sparkSession.sparkContext.broadcast(set(edgeList))\n",
    "\n",
    "# define udf\n",
    "from pyspark.sql.functions import udf\n",
    "def isInEdgeList(src, dst):\n",
    "    return (int(src), int(dst)) in edgeListBC.value\n",
    "\n",
    "udf_isInEdgeList = udf(isInEdgeList, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+\n",
      "|  B|  A| C1|isTriangle|\n",
      "+---+---+---+----------+\n",
      "|  3|  1|  4|      true|\n",
      "|  3|  1|  5|     false|\n",
      "|  3|  1|  6|     false|\n",
      "|  3|  1|  7|     false|\n",
      "|  3|  2|  4|     false|\n",
      "|  3|  2|  5|      true|\n",
      "|  3|  2|  6|     false|\n",
      "|  3|  2|  7|     false|\n",
      "|  2|  1|  3|      true|\n",
      "|  2|  1|  5|     false|\n",
      "+---+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abc1.withColumn('isTriangle', udf_isInEdgeList('A', 'C1')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
