{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Object\n",
    "from pyspark.sql import SparkSession\n",
    "spark_session = SparkSession \\\n",
    "                    .builder \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .appName(\"spark sql\") \\\n",
    "                    .master(\"local\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rdd from a text file\n",
    "geoip_rdd = spark_session \\\n",
    "                .sparkContext \\\n",
    "                .textFile(\"/user/pmezentsev/geoip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 1st 3 rows\n",
    "geoip_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "geoip_rdd1 = geoip_rdd.map(lambda x: x.split(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType() \\\n",
    "            .add(\"ip\", StringType()) \\\n",
    "            .add(\"code\", StringType()) \\\n",
    "            .add(\"country\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RDD -> schema = spark dataframe\n",
    "geoip_df = spark_session \\\n",
    "                .createDataFrame(geoip_rdd1, schema)\n",
    "    \n",
    "# show dataframe\n",
    "geoip_df.show(3)\n",
    "\n",
    "# check rdd of dataframe\n",
    "geoip_df.rdd.take(2)\n",
    "\n",
    "# print schema\n",
    "geoip_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark dataframe -> pandas dataframe(careful may run out of memory)\n",
    "geoip_pd = geoip_df.toPandas()\n",
    "geoip_pd.head(3)\n",
    "\n",
    "# pandas dataframe -> spark dataframe\n",
    "geoip_df = spark_session.createDataFrame(geoip_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process df as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql\n",
    "geoip_df \\\n",
    "    .select(\"country\", \"ip\") \\\n",
    "    .where(\"country = 'Russian Federation'\") \\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SQL view (SQL keeps the data - SQL view recompute data upon request)\n",
    "geoip_df.createTempView(\"geoip\")\n",
    "\n",
    "# sql from view\n",
    "spark_session.sql(\"\"\"\n",
    "    select country from geoip\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working w/ Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List in pandas\n",
    "## show database\n",
    "spark_session.sql(\"\"\"\n",
    "    show databases\n",
    "\"\"\").toPandas()\n",
    "\n",
    "## show tables(in web database)\n",
    "spark_session.sql(\"\"\"\n",
    "    show tables in web\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "## list all databases\n",
    "spark_session.catalog.listDatabases()\n",
    "\n",
    "## show tables(in web database)\n",
    "spark_session.catalog.listTables(\"web\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tables from spark sql -> hive\n",
    "spark_session.sql(\"\"\"\n",
    "    create table\n",
    "        web.geoip as\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        geoip\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a table -> hive\n",
    "geoip_df.write.saveAsTable(\"web.geoip\", mode='overwrite')\n",
    "\n",
    "# save a table -> file\n",
    "geoip_df.write.save(\"geoip\",format='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD vs spark df vs SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF > SQL > RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection(columns) and filtering(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        ip, url\n",
    "    from\n",
    "        web.access_log\n",
    "\"\"\").limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark df\n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "access_log_df \\\n",
    "    .select(\"ip\", \"url\") \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# or\n",
    "access_log_df \\\n",
    "    .select(access_log_df.ip, access_log_df.url) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# or\n",
    "access_log_df \\\n",
    "    .[[\"url\", \"ip\"]] \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# change column names\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        access_log_df.ip,\n",
    "        access_log_df.url.alias(\"url_part\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Use functions\n",
    "import pyspark.sql.functions as f\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        f.col(\"ip\"),\n",
    "        f.col(\"url\").alias(\"url_part\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        web.access_log\n",
    "    where\n",
    "        http_code <> '200'\n",
    "\"\"\").limit(3).toPandas()\n",
    "\n",
    "# spark df\n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "access_log_df \\\n",
    "    .where(\"http_code <> '200'\") \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# or\n",
    "access_log_df \\\n",
    "    .where(access_log_df.http_code <> '200') \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        web.access_log\n",
    "    where\n",
    "        http_code <> '200' and\n",
    "        user_agent like '%Android%'\n",
    "\"\"\").limit(3).toPandas()\n",
    "\n",
    "# DF\n",
    "access_log_df \\\n",
    "    .where(\n",
    "        (access_log_df.http_code <> '200') &\n",
    "        (access_log_df.user_agent.like('%Android%'))\n",
    "    ) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions\n",
    "map: n -> n\n",
    "generating: n -> m (n<m)\n",
    "aggregating: m -> n (m>n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        user_agent,\n",
    "        length(user_agent) as len\n",
    "    from\n",
    "        web.access_log\n",
    "    limit\n",
    "        3\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# spark df\n",
    "import pyspark.sql.functions as f \n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        \"user_agent\",\n",
    "        f.length(\"user_agent\").alias(\"len\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        \"url\",\n",
    "        f.concat(f.lit(\"http://vk.com\"), access_log_df.url)\n",
    "    )\\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "access_log_df \\\n",
    "    .select(\"user_agent\") \\\n",
    "    .select(\n",
    "        \"user_agent\",\n",
    "        f.split(\"user_agent\", \" \").alias(\"list\")\n",
    "    ) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode (seperate -> list)\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        \"user_agent\",\n",
    "        f.split(\"user_agent\", \" \").alias(\"list\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"user_agent\",\n",
    "        f.explode(\"list\")\n",
    "    ) \\\n",
    "    .where(f.col(\"col\") == \"Android\") \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when, otherwise\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        \"user_agent\",\n",
    "        f \\\n",
    "            .when(access_log_df.user_agent.like(\"%Android%\"), \"Android\") \\\n",
    "            .otherwise(\"Other\") \\\n",
    "            .alias(\"OS\")\n",
    "    ) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        url,\n",
    "        count(ip)\n",
    "    from\n",
    "        web.access_log\n",
    "    group by\n",
    "        url\n",
    "    limit\n",
    "        3\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# spark df\n",
    "import pyspark.sql.functions as f \n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "access_log_df \\\n",
    "    .groupBy(\"url\") \\\n",
    "    .agg(f.count(\"ip\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()\n",
    "\n",
    "# or\n",
    "access_log_df \\\n",
    "    .groupBy(\"url\") \\\n",
    "    .agg({\"ip\": \"count\"}) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        url,\n",
    "        http_code,\n",
    "        count(distinct ip)\n",
    "    from\n",
    "        web.access_log\n",
    "    group by\n",
    "        url,\n",
    "        http_code\n",
    "    limit\n",
    "        3\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# spark df\n",
    "access_log_df \\\n",
    "    .groupBy(\"url\", \"http_code\") \\\n",
    "    .agg(f.count(\"ip\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark df\n",
    "access_log_df \\\n",
    "    .groupBy(f.length(\"url\")) \\\n",
    "    .agg(f.count(\"ip\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg whole table n --> 1\n",
    "access_log_df \\\n",
    "    groupBy() \\\n",
    "    .agg(f.count(\"*\")) \\\n",
    "    .limit(3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count: find the most frequent word\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        f.split(\"user_agent\", \" \").alias(\"words\")) \\\n",
    "    .select(\n",
    "        f.explode(\"words\").alias(\"words\")) \\\n",
    "    .groupBy(\n",
    "        \"word\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\n",
    "        f.col(\"count\").desc()) \\\n",
    "    .limit(\n",
    "        3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        web.access_log as l\n",
    "    join\n",
    "        web.geoip as g\n",
    "    on\n",
    "        l.ip = g.ip\n",
    "\"\"\").limit(3).toPandas()\n",
    "\n",
    "# spark df\n",
    "import pyspark.sql.functions as f \n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "access_log_df \\\n",
    "    .join(\n",
    "        geoip_df, on = (access_log.ip == geoip.ip)) \\\n",
    "    .limit(\n",
    "        3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_log_df \\\n",
    "    .join(\n",
    "        geoip_df, on = \"ip\") \\\n",
    "    .groupby(\n",
    "        \"country\") \\\n",
    "    .agg(\n",
    "        f.countDistinct(\"ip\").alias(\"cnt\")) \\\n",
    "    .limit(\n",
    "        3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join\n",
    "access_log_df \\\n",
    "    .join(\n",
    "        geoip_df, on = \"ip\", how = \"left\") \\\n",
    "    .limit(\n",
    "        3) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "import pyspark.sql.functions as f \n",
    "\n",
    "access_log_df = spark_session.read.table(\"web.access_log\")\n",
    "\n",
    "def parse_user_agent_udf(user_agent):\n",
    "    user_agent = re.sub(\"/?[\\d_.]+\", \"\", user_agent) # remove numbers, points, slashes, uderlines\n",
    "    user_agent = re.sub(\"[;\\(\\):,]\", \"\", user_agent) # remove punctuation marks, opening brackets, closing brackets, periods, semi-colons.\n",
    "    return user_agent.lower().split()\n",
    "\n",
    "\n",
    "access_log_df \\\n",
    "    .select(\n",
    "        parse_user_agent_udf(\"user_agent\").alias(\"words\")) \\\n",
    "    .select(\n",
    "        f.explode(\"words\").alias(\"word\")) \\\n",
    "    .groupBy(\n",
    "        \"word\") \\\n",
    "    .agg(\n",
    "        f.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\n",
    "        f.col(\"count\").desc()) \\\n",
    "    .limit(\n",
    "        3) \\\n",
    "    .toPandas()\n",
    "    \n",
    "\n",
    "# user defined functions SQL\n",
    "# SQL\n",
    "spark_session.sql(\"\"\"\n",
    "    select\n",
    "        word,\n",
    "        count(*) as cnt\n",
    "    from (\n",
    "        select\n",
    "            explode(parse_user_agent_udf(user_agent)) as word\n",
    "        from\n",
    "            web.access_log\n",
    "    ) as s\n",
    "    group by \n",
    "        word\n",
    "    order by\n",
    "        cnt desc\n",
    "\"\"\").limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time\n",
    "access_log_unix_time \\\n",
    "    .withColumn(\n",
    "        \"unixtime\",\n",
    "        f.unix_timestamp(\"time\", \"dd/MMM/yyyy:HH:mm:ss Z\")) \\\n",
    "    .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "access_log_unix_time \\\n",
    "    .groupby(\n",
    "        \"ip\") \\\n",
    "    .agg(\n",
    "        f.min(\"unixtime\").alias(\"begin\"),\n",
    "        f.max(\"unixtime\").alias(\"end\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"ip\",\n",
    "        (f.col(\"end\") - f.col(\"begin\"))\n",
    "    ).alias(\"seconds_cnt\") \\\n",
    "    .select(\n",
    "        \"ip\",\n",
    "        f.col(\"seconds_cnt\")/60.0/60.0/24.0 + 1\n",
    "    ) \\\n",
    "    .limit(\n",
    "        5) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unixtime -> timestamp\n",
    "access_log_timestamp = access_log_unix_time \\\n",
    "    .withColumn(\n",
    "        \"timestamp\",\n",
    "        f.col(\"unixtime\").astype(\"timestamp\")\n",
    "    ) \\\n",
    "    .limit(\n",
    "        5) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Note: datediff only work w/ timestamp\n",
    "access_log_timestamp \\\n",
    "    .groupby(\n",
    "        \"ip\") \\\n",
    "    .agg(\n",
    "        f.min(\"timestamp\").alias(\"begin\"),\n",
    "        f.max(\"timestamp\").alias(\"end\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"ip\",\n",
    "        (f.datediff(\"end\", \"begin\")).alias(\"days_cnt\")\n",
    "    ) \\\n",
    "    .limit(\n",
    "        5) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows functions\n",
    "    - special kind of aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |  aggregation | window function |\n",
    "|:--- :|:---:| :---:|\n",
    "| applied to | whole table | column  |\n",
    "| number of rows | reduces | remains unchanged |\n",
    "| grouping condition | goes first | goes last |\n",
    "| values in a group | df.groupby(...).agg(...) <br> unordered | func(\"column\").over(...) <br> ordered |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_log_timestamp \\\n",
    "    .select(\n",
    "        \"ip\",\n",
    "        \"time\",\n",
    "        f.count(\"*\").over(Window.partitionBy(\"ip\")).alias(\"cnt\")\n",
    "    ) \\\n",
    "    .limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-D distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
