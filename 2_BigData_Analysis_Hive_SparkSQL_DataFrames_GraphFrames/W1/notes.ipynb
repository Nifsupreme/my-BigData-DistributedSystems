{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Use Cases DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Most popular region\n",
    "SELECT regioncity, COUNT(1) AS hit_count\n",
    "FROM access_log \n",
    "    JOIN geo_base ON(access_log.host = geo_base.host)\n",
    "GROUP BY regioncity ORDER BY hit_count LIMIT 100\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Real users vs bots distribution\n",
    "SELECT request,\n",
    "    SUM(IF(robot.bot_name IS NULL, 1, 0)) AS user_hit_count,\n",
    "    SUM(IF(robot.bot_name IS NULL, 1, 0)) AS user_hit_count\n",
    "FROM access_log LEFT OUTER \n",
    "    JOIN robot ON(\n",
    "        access_log.host = robot.host AND\n",
    "        access_log.user_agent = robot.user_agent\n",
    "    )\n",
    "GROUP BY request\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Male vs female audience (per region)\n",
    "SELECT regioncity,\n",
    "    SUM(IF(user.gender = \"M\", 1, 0)) AS male_hit_count,\n",
    "    SUM(IF(user.gender = \"F\", 1, 0)) AS female_hit_count\n",
    "FROM access_log\n",
    "    JOIN geo_base ON (access_log.host = geo_base.host)\n",
    "    JOIN user ON(\n",
    "        access_log.host = user.host AND\n",
    "        access_log.user_agent = user.user_agent\n",
    "    )\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average customer age\n",
    "SELECT region, AVG(user.age)\n",
    "FROM access_log\n",
    "    JOIN geo_base ON(access_log.host = geo_base.host)\n",
    "    JOIN user ON(\n",
    "        access_log.host = user.host\n",
    "        access_log.user_agent = user.user_agent\n",
    "    )\n",
    "GROUP BY region\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "SELECT *\n",
    "FROM Customers\n",
    "    WHERE Country='Mexico'AND PostalCode='05023';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by \n",
    "SELECT *\n",
    "FROM Customers\n",
    "ORDER BY Country\n",
    ";\n",
    "\n",
    "SELECT *\n",
    "FROM Customers\n",
    "ORDER BY Country DESC\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join\n",
    "SELECT \n",
    "    Orders.OrderID,\n",
    "    Customers.CustomerName,\n",
    "    Orders.OrderDate\n",
    "FROM Orders\n",
    "    JOIN Customers ON Orders.CustomerID = Customers.CustomerID\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation\n",
    "SELECT <column_name>, <agg_functions>(column_name)\n",
    "FROM <table_name>\n",
    "GROUP BY <column_name>\n",
    ";\n",
    "\n",
    "\n",
    "SELECT Country, COUNT(*)\n",
    "FROM Customers\n",
    "GROUP BY Country\n",
    ";\n",
    "SELECT Country, State, COUNT(*)\n",
    "FROM Customers\n",
    "GROUP BY Country, State\n",
    ";\n",
    "SELECT OrderDate, MIN(OrderID), MAX(OrderID)\n",
    "FROM Orders\n",
    "GROUP BY OrderDate\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where: filter data before aggregation\n",
    "# Having: filter data after aggregation (more resource consumption)\n",
    "SELECT <column_name>, <agg_functions>(column_name)\n",
    "FROM <table_name>\n",
    "    WHERE <column_name> <operator> <value>\n",
    "GROUP BY <column_name>\n",
    "    HAVING <agg_functions>(column_name)\n",
    ";\n",
    "\n",
    "SELECT Country\n",
    "FROM Cusomers\n",
    "GROUP BY Country\n",
    "    HAVING COUNT(*) > 10\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subqueries\n",
    "SELECT *\n",
    "FROM Customers\n",
    "    WHERE Country IN(\n",
    "        SELECT Country\n",
    "        FROM Customers\n",
    "        GROUP BY Country\n",
    "            HAVING COUNT(*) > 10\n",
    "    )\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Data Definition Language (DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table\n",
    "CREATE TABLE my_table_name(\n",
    "    dummy_column STRING,\n",
    "    another_column STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe database: check HDFS location\n",
    "DESCRIBE DATABASE default;\n",
    "DESCRIBE TABLE[EXTENDED|FORMATTED] <table_name>;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table from an existing HDFS dataset\n",
    "USE <database_name>;\n",
    "CREATE TABLE <table_name>(\n",
    "    <column_name> <column_type>,\n",
    "    ...\n",
    ")\n",
    "LOCATION \"path/to/hdfs\"\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default table = managed table: drop table = metadata + data in HDFS removed\n",
    "# external table: drop table = metadata removed, data in HDFS unchanged\n",
    "# temporary table: auto removed after a hive session closed\n",
    "%% tab_dataset/part-*\n",
    "first   line    1\n",
    "second  line    3\n",
    "last    line    5\n",
    "##############################################\n",
    "USE mydb;\n",
    "DROP TABLE IF EXIST tab_dataset;\n",
    "CREATE EXTERNAL TABLE tab_dataset(\n",
    "    first_column STRING,\n",
    "    second_column STRING,\n",
    "    value INT\n",
    ")\n",
    "ROW FORMAT delimited\n",
    "    fields terminated by '\\t'\n",
    "LOCATION 'path/to/tab_dataset'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex type\n",
    "%% dataset/part-*\n",
    "John Doe'^A'10000.0'^A'Mary Smith'^B'Todd Jones'^A'Federal Taxes'^C'.2'^B'State Taxes'^c'.05'^B'Insurance'^C'.1'^A'Michigan Avenue.'^B'Chicago'^B'IL'^B'60600\n",
    "############################################\n",
    "CREATE TABLE employees(\n",
    "    name STRING,\n",
    "    salary FLOAT,\n",
    "    subordinates ARRAY<STRING>,\n",
    "    deductions MAP<STRING, FLOAT>,\n",
    "    address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>\n",
    ")\n",
    "ROW FORMAT delimited\n",
    "    fields terminated by '\\001' #^A\n",
    "    collection items terminated by '\\002' #^B\n",
    "    map keys terminated by '\\003' #^C\n",
    "    lines terminated by '\\n'\n",
    "STORED AS <file_format>\n",
    "LOCATION 'path/to/dataset'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Data Manipulation Language(DML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DML import from HDFS -> Hive\n",
    "LOAD DATA INPATH '/path/employees-data'\n",
    "INTO TABLE employees\n",
    ";\n",
    "\n",
    "# DML import from local FS -> Hive\n",
    "LOAD DATA LOCAL INPATH '/path/employees-data'\n",
    "INTO TABLE employees\n",
    ";\n",
    "\n",
    "# Replace data in tables\n",
    "LOAD DATA [LOCAL] INPATH '/path/employees-data'\n",
    "OVERWRITE INTO TABLE employees\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DML export Hive -> local or HDFS\n",
    "INSERT OVERWRITE [LOCAL] DIRECTORY 'tmp/employees'\n",
    "SELECT name, salary, address\n",
    "FROM employees\n",
    "    WHERE ...\n",
    ";\n",
    "\n",
    "# DML export multiple insert\n",
    "INSERT OVERWRITE [LOCAL] DIRECTORY 'tmp/ca_employees'\n",
    "SELECT name, salary, address\n",
    "FROM employees\n",
    "    WHERE state = 'CA'\n",
    "INSERT OVERWRITE [LOCAL] DIRECTORY 'tmp/ny_employees'\n",
    "SELECT name, salary, address\n",
    "FROM employees\n",
    "    WHERE state = 'NY'\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move tables -> tables\n",
    "FROM raw_table\n",
    "INSERT OVERWRITE TABLE us_employees\n",
    "SELECT *\n",
    "WHERE raw_table.country = 'US'\n",
    "INSERT OVERWRITE TABLE uk_employees\n",
    "SELECT *\n",
    "WHERE raw_table.country = 'UK'\n",
    "...\n",
    ";\n",
    "\n",
    "# Create tables from tables\n",
    "CREATE TABLE ca_employees\n",
    "AS SELECT name, salary, address\n",
    "FROM employees\n",
    "WHERE state = 'CA'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT ... FROM (<-- MAP)\n",
    "WHERE (<-- MAP)\n",
    "GROUP BY (<-- Shuffle and Sort)\n",
    "HAVING (<-- Reduce)\n",
    "JOIN (<-- Map, Reduce)\n",
    "ORDER BY / SORT BY (<-- Reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%log\n",
    "127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200\n",
    "123.65.150.10 - - [23/Aug/2010:03:50:59 +0000] \"POST /wordpress3/wp-admin/admin-ajax.php HTTP/1.1\" 200\n",
    "#######################################\n",
    "CREATE EXTERNAL TABLE apache_log(\n",
    "    ip STRING,\n",
    "    auth_unused STRING,\n",
    "    auth_user STRING,\n",
    "    request_time STRING,\n",
    "    request STRING,\n",
    "    status_code INT\n",
    ")\n",
    "ROW FORMAT\n",
    "    SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "    WITH SERDEPROPERTIES (\n",
    "        \"input.regex\" = '^(\\\\S*) (\\\\S*) (\\\\S*) \\\\[([^\\\\]]*)\\\\] \"([^\"]*)\" (\\\\S*) .*$' # string,string,string,[string],\"string\",string\n",
    "    )\n",
    "LOCATION 'path/to/log'\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View - Read only Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view table from an existing table\n",
    "CREATE VIEW apache_log_view(\n",
    "    ip,\n",
    "    request_year,\n",
    "    request,\n",
    "    status_code\n",
    ")\n",
    "AS SELECT\n",
    "    ip,\n",
    "    regexp_extract(request_time, \"\\\\d+\\\\/\\\\w+\\\\/(\\\\d+)\", 1), # 10/Oct/2000:13:55:36-0700 -> 2000\n",
    "    request,\n",
    "    status_code\n",
    "FROM apache_log\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import re\n",
    "re.search('\\\\d+\\\\/\\\\w+\\\\/(\\\\d+)', '10/Oct/2000:13:55:36-0700').group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Analytics (UDF, UDAF, UDTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Operators\n",
    "    =, !=, <, >, IS NULL, +, - , *, /, AND, OR, IN ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Functions (UDFs = User Defined Functions)   # map 1 -> 1\n",
    "    math: round, floor, ceil, exp, log...\n",
    "    date: to_date, from_unixtimestamp, year...\n",
    "    conditional: if, isnull, case...\n",
    "    string: char, concat, lower, trim, repeat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Aggregate functions (UDAFs) # map n -> 1\n",
    "    count, sum, min, max, corr..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Table-generating functions (UDTFs) # map 1->n\n",
    "    explode, posexplode, parse_url_tuple..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word counts by Hive\n",
    "ADD FILE /path/to/mapper.py\n",
    "ADD FILE /path/to/reducer.py\n",
    "\n",
    "FROM(\n",
    "    FROM wikipedia_sample\n",
    "    SELECT TRANSFORM(line)\n",
    "    USING \"./mapper.py\" AS word, counts\n",
    "    DISTRIBUTE BY word SORT BY word\n",
    ") word_pairs\n",
    "SELECT TRANSFORM(word_pairs.word, word_pairs.counts)\n",
    "USING \"./reducer.py\"\n",
    "AS word, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Partitioned Table Functions (PTF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "FROM transactions\n",
    "    SELECT TRANSFORM(customerID, change)\n",
    "    USING \"./locate_overdraft.py\"\n",
    "DISTRIBUTE BY customerID\n",
    "SORT BY customerID, timestamp\n",
    "\n",
    "# PTF function map n->1\n",
    "SELECT\n",
    "    customerID,\n",
    "    transactionID,\n",
    "    change,\n",
    "    SUM(change) OVER(\n",
    "        PARTITION BY customerID\n",
    "        ORDER BY transactionID\n",
    "    )\n",
    "FROM transactions\n",
    "SORT BY customerID, transactionID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "SELECT region, COUNT(1) AS hit_count\n",
    "FROM access_log \n",
    "    JOIN geo_base ON(access_log.host = geo_base.host)\n",
    "WHERE access_log.datetime BETWEEN \"recent_date\" AND \"now\"\n",
    "GROUP BY region\n",
    "ORDER BY hit_count LIMIT 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules:\n",
    "    1. partitioned columns stay at the ends of SELECT\n",
    "    2. partitioned columns must be specified in order\n",
    "    3. Use configuration parameters\n",
    "    4. Control empty partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data: reduce the computational data processing\n",
    "# create table\n",
    "CREATE TABLE partitioned_access_log(\n",
    "    ip STRING,\n",
    "    ...\n",
    ")\n",
    "PARTITIONED BY(year STRING, month STRING, day STRING)\n",
    "...\n",
    ";\n",
    "\n",
    "# transfer data\n",
    "SET hive.exec.max.dynamic.partitions=2048;\n",
    "SET hive.exec.max.dynamic.partitions.pernode=256;\n",
    "SET hive.exec.max.created.files=10000;\n",
    "SET hive.error.on.empty.partition=true;\n",
    "\n",
    "FROM raw_access_log\n",
    "INSERT OVERWRITE TABLE partitioned_access_log\n",
    "PARTITION(year=\"2017\", month=\"03\", day=\"25\")\n",
    "SELECT ip, ..., year, month, day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buckets: the number of part-* files in HDFS folder\n",
    "CREATE TABLE granular_access_log(\n",
    "    ip STRING,\n",
    "    ...\n",
    ")\n",
    "PARTITIONED BY(request_date STRING)\n",
    "CLUSTERED BY(user_id, ...) SORTED BY(user_id) INTO 200 BUCKETS\n",
    "...\n",
    ";\n",
    "\n",
    "# Sampling: process x% of all buckets\n",
    "SELECT ip,...\n",
    "FROM granular_access_log\n",
    "    TABLESAMPLE(BUCKET 1 OUT OF 4 ON user_id) # 25%\n",
    "...\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template 1\n",
    "SET mapred.reduce.tasks = 200;\n",
    "\n",
    "FROM raw_access_log\n",
    "INSERT OVERWRITE TABLE granular_access_log\n",
    "PARTITION BY(request_date)\n",
    "SELECT ..., request_date\n",
    "WHERE ...\n",
    "DISTRIBUTE BY user_id\n",
    "[SORT BY user_id]\n",
    ";\n",
    "\n",
    "# Template 2\n",
    "SET hive.enforce.bucketing = true;\n",
    "\n",
    "FROM raw_access_log\n",
    "INSERT OVERWRITE TABLE granular_access_log\n",
    "PARTITION BY(request_date)\n",
    "SELECT ..., request_date\n",
    "WHERE ...\n",
    ";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
